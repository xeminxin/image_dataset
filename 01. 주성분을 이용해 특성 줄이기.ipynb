{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318247c5",
   "metadata": {},
   "source": [
    "# 주성분을 사용해 특성 줄이기\n",
    "- PCA는 데이터의 분산을 유지하면서 특성의 수를 줄이는 선형 차원 축소 기법이다.\n",
    "- 타깃 벡터의 정보를 사용하지 않고 특성 행렬만 이용하는 비지도 학습 기법이다.\n",
    "- 데이터는 두 개의 특성 x1과 x2를 가진다.\n",
    "- 그래프의 샘플들은 길이가 길고 높이는 낮은 타원 모양으로 퍼져있는 '길이' 방향의 분산이 '높이' 방향보다 훨씬 크다.\n",
    "- 길이와 높이 대신 가장 분산이 많은 방향을 첫 번째 주성분으로 부르고 두 번째로 가장 많은 방향을 두 번째 주성분이라고 부른다.\n",
    "- 특성을 줄이는 한 가지 방법은 2D 공간의 모든 샘플을 1차원 주성분에 투영하는 것이다.\n",
    "- n_components의 입력 매개변수값이 1보다 크면 n_components 개수만큼의 특성이 반환\n",
    "- n_componentsfmf 0과 1 사이로지정하면 pca는 해당 비율의 분산을 유지할 수 있는 최소한의 특성 개수를 반환 \n",
    "- 원본 특성의 95%와 99%의 분산을 유지하는 0.95와 0.99가 사용됨\n",
    "- whiten=True로 지정하면 각 주성분의 값을 평균이 0이고 분산이 1이 되도록 변환\n",
    "- solver='randomized'는 아주 짧은 시간 안에 첫 번째 주성분을 찾아주는 확률적 알고리즘을 사용\n",
    "- 화이트닝은 주성분에 투영된 특성의 스케일을 맞추는 역할을 한다.\n",
    "- PCA는 평균을 0으로 맞추기 때문에 화이트닝 옵션 대신 나중에 투영된 특성을 표준화해도 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b495ad51",
   "metadata": {},
   "source": [
    "# 사이킷런 손글씨 데이터를 활용하여 특성 행렬을 표준화 처리 및 주성분 특성 줄이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eeda7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주성분을 사용해 특성 줄이기 실습\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1be2b403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.33501649 -0.04308102 ... -1.14664746 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...  0.54856067 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...  1.56568555  1.6951369\n",
      "  -0.19600752]\n",
      " ...\n",
      " [ 0.         -0.33501649 -0.88456568 ... -0.12952258 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -0.67419451 ...  0.8876023  -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649  1.00877481 ...  0.8876023  -0.26113572\n",
      "  -0.19600752]]\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()      # 8x8 크기의 손글씨 숫자 데이터 로드\n",
    "feature = StandardScaler().fit_transform(digits.data)\n",
    "# fit_transform : 데이터 표준화 후, 변환된 값을 반환\n",
    "\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c20ad79",
   "metadata": {},
   "source": [
    "1. StandardScaler() \n",
    "> Scikit-learn의 전처리(preprocessing) 모듈에서 제공되는 클래스 중 하나다. 이 클래스는 데이터를 평균이 0, 분산이 1인 가우시안 정규 분포(standard normal distribution)로 변환한다.\n",
    " \n",
    "2. digits.data : digits 데이터셋에서 숫자 이미지의 각 픽셀 값을 포함하는 배열\n",
    " \n",
    "3. fit_transform() 메서드 \n",
    "> StandardScaler 클래스에는 데이터를 변환하는 두 가지 단계가 있다. 첫째, 모델을 학습(fit)하고, 둘째, 학습된 모델을 사용하여 데이터를 변환(transform)한다. fit_transform() 메서드는 이 두 단계를 한 번에 수행합니다. 즉, 데이터를 표준화(normalize)하고, 변환된 값을 반환한다.\n",
    "따라서 위의 코드는 digits 데이터셋의 특성을 가우시안 정규 분포로 변환한 후, 변환된 값을 featuress 변수에 할당 이렇게 정규화를 수행하면, 모델이 데이터를 더 잘 이해하고, 모델의 예측 성능을 향상 시킬 수 있다.\n",
    "\n",
    "따라서 위의 코드는 digits 데이터셋의 특성을 가우시안 정규 분포로 변환한 후, 변환된 값을 featuress 변수에 할당 이렇게 정규화를 수행하면, 모델이 데이터를 더 잘 이해하고, 모델의 예측 성능을 향상 시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f11c4",
   "metadata": {},
   "source": [
    "## 99%의 분산을 유지하도록 PCA 클래스 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61fb699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 특성 개수 >>  64\n",
      "줄어든 특성 개수 >>  54\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn의 PCA 클래스를 이용하여, 데이터셋의 차원을 감소시키는 예시이다.\n",
    "# 99%의 분산을 유지하도록 PCA 클래스 객체 생성\n",
    "pca = PCA(n_components=0.99, whiten=True)\n",
    "features_pca = pca.fit_transform(feature)     # PCA 수행\n",
    "\n",
    "# result\n",
    "print(\"원본 특성 개수 >> \", feature.shape[1])\n",
    "print(\"줄어든 특성 개수 >> \", features_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395401d3",
   "metadata": {},
   "source": [
    "PCA 클래스: Scikit-learn의 decomposition 모듈에서 제공되는 클래스 중 하나다. PCA는 데이터셋의 차원을 감소시키는 기술로, 데이터셋에서 가장 중요한 특성만 추출하여 새로운 차원 축으로 변환한다. 이를 통해 데이터셋의 노이즈(noise)를 제거하고, 더욱 빠르고 효율적인 학습이 가능해진다.<br><br>\n",
    "n_components: PCA 클래스의 인자 중 하나로, 추출할 주성분(principal component)의 수를 지정합니다. 여기서는 99%의 분산(variance)을 유지하도록 설정되어 있다. 이는 데이터셋에서 99%의 정보가 유지되도록 차원을 축소하는 것을 의미한다.<br><br>\n",
    "whiten: PCA 클래스의 인자 중 하나로, True로 설정할 경우 PCA의 결과로 나오는 주성분들이 서로 독립적인 값이 되도록 백색화(whitening)를 수행한다. 백색화를 하면 각 주성분의 분산이 1이 되고, 상관 관계가 없는 성분들로 구성된 새로운 특성 공간이 만들어진다.<br><br>\n",
    "fit_transform(): PCA 클래스에는 fit()과 transform() 메서드가 있다. fit() 메서드는 PCA 모델을 학습하고, transform() 메서드는 학습된 모델을 사용하여 데이터를 변환한다. fit_transform() 메서드는 이 두 단계를 한 번에 수행한다.<br><br>\n",
    "위의 같이 PCA를 이용하면 99%의 분산을 유지하도록 새로운 특성(feature) 공간으로 변환하고 있다. 결과적으로, 원본 데이터셋의 특성 개수는 features.shape[1]으로 확인할 수 있고, PCA를 수행하여 감소된 특성 개수는 features_pca.shape[1]으로 확인할 수 있다. 이렇게 차원 축소를 수행하면, 모델의 학습 시간을 단축시키고, 과적합(overfitting)을 방지할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aa1a70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
