# image_dataset
이미지 다루기 및 데이터 셋 구축

# 01. 다양한 데이터 처리 방법

- 데이터를 전처리 하는 이유
1. 머신러닝 파이프라인에서, 데이터는 모델 학습 및 서빙의 입력에 알맞게 가공되어야 한다. -> 전처리
2. 실제 데이터는 다양한 소스 및 프로세스가 수집되며 데이터 세트의 품질을 떨어트리는 이상값, 또는 손상값이 포함될 수 있다. 이를 해결하기 위해 데이터 정리 및 전처리가 필요하다.
3. 특정 분석에 적합하게 데이터를 가공하는 작업이다.
4. 현업 같은 데서 발생하는 데이터는 분석, ML, DL에 적합하지 않은 경우가 있다. 의미 없는 값 혹은 존재하지 않는 값, 수많은 변수는 데이터의 품질을 떨어트린다. 이를 방지하기 위한 작업이다.

- Data Pipeline이란
언제든지 필요한 데이터를 가져와 꺼내쓸 수 있도록 데이터를 계속 쌓아두는 파이프를 만드는 것

- Data Pipeline 사용 예시
데이터 기반 의사결정 (Data Driven)이 대세이다.
데이터 드리븐은 데이터를 기반으로 핵심 의사결정을 하게 된다.

- Data Pipeline 하는 일
1. 데이터 생성
2. 데이터 수집
3. 데이터 가공 후 저장
4. 데이터 시각화(Bl)

- 데이터 웨어하우스 (Data Warehouse)
- 방대한 조직 내에서 분산 운영되는 각각의 데이터 베이스 관리 시스템들을 효율적으로 통합하여 조정 관리하는 것

- 데이터 레이크
구조화되거나 반구조화되거나 구조화되지 않은 대량의 데이터를 저장, 처리, 보호하기 위한 중앙 집중식 저장소. 데이터 레이크는 확장 가능하고 안전한 플랫폼을 제공한다.

- 데이터 레이크와 데이터 웨어하우스 비교
데이터 레이크는 현재 정의된 용도가 없는 비정형 원시 데이터를 저장한다. 데이터 웨어하우스는 어떤 데이터를 포함할지 저장 전에 결정하게 되는데, 이를 쓰기 스키마라고 한다. 이 작업은 굉장히 오래 걸리는 편이라, 즉시 데이터를 수집할 수 없다. 반면 데이터 레이크를 활용하면 즉시 데이터를 수집하여 향후 해당 데이터를 어디에 사용할지 파악힐 수 있다.

- 데이터 분석 프로세스
1. 문제 정의
2. 데이터 수집
3. 데이터 전처리
4. 모델링
5. 해석 및 시각화

- 데이터 타입
범주형 변수 & 수치해석학

- 데이터 format
* 정형 데이터 : 데이터베이스에 사전에 정의된 규칙에 맞게 들어간 데이터 중에 수치만으로 의미 파악이 쉬운 데이터를 의미 / 데이터 스키마 지원
* 비정형 데이터 : 정형 데이터와 반대되는 단어. 비정형 데이터는 형태가 없고, 연산이 불가능한 데이터를 의미한다. 정해진 규칙이 없기 때문에 데이터의 의미를 파악하기 힘들다. Data set이 아닌 하나의 데이터가 객체화 되어있다. 구조화가 아니라서 이해하기 힘들다. 이진 파일 형태 데이터이면 종류별로 응용 SW를 이용하여 탐색한다.'
* 반정형 데이터 : Semi를 의미. 즉 완전한 정형이 아닌 약한 정형 데이터이다. 데이터베이스는 아니지만 스키마를 가지고 있는 형태이며, 연산이 불가능한 데이터이다. 스키마에 해당하는 메타 데이터가 데이터 내부에 존재한다. 데이터 내부에 있는 규칙성을 파악하여 parsing 규칙 적용

- 데이터 전처리 필요성 및 방법
  * 이상값의 원인 : 인공(오류) / 비자연적, 자연적

- 다양한 유형의 이상값
1. 데이터 오류 : 데이터 수집, 기록 또는 입력 중 발생하는 오류와 같은 인적 오류
2. 측정 오류 : 측정 기기에 결함이 있는 경우
3. 실험 오류 : 실험 진행 상황에서 오류가 있는 경우
4. 의도적 이상치 : 일반적으로 민감한 데이터와 관련된 자체보고 측정에서 발견됨
5. 데이터 처리 오류 : 데이터 마이닝을 수행할 때마다 여러 소스에서 데이터를 추출한다. 일부조작 또는 추출 오류로 인해 데이터 세트에서 이상값이 발생할 수 있다.
6. 샘플링 오류 : 고양이 이미지 폴더에 다른 이미지가 포함된 경우
7. 자연적 이상값 : 이상값이 인위적이지 않은 경우

- 데이터 전처리 방법
1. 데이터 클리닝
   - 결측치 대체
   - 잡음 데이터의 평활
   - 이상치의 확인 및 제거
   - 불일치 해결
2. 데이터 통합
   - 다양한 로그 파일 및 데이터베이스의 통합
   - 일관성 있는 데이터 형태로 변환
3. 데이터 변환
   - 정규화
   - 집합화
   - 요약
   - 계층 생성
4. 데이터 축소
   - 축소된 데이터도 원래 데이터와 같은 분석 결과를 얻을 수 있어야 한다.
5. 데이터 이산화
   - 수치값을 속성값으로 변환
6. 데이터 표현 특징 추출
   - 데이터를 더 잘 이해하기 위해서 대표 특징을 이해하는 과정
   - 데이터 축소의 일종이기도 함

- 결측치 처리 방법
1. 해당 튜플 무시 : 주로 분류 문제에서 클래스 구분 라벨이 빠진 경우
2. 기준에 따라 자동으로 채우기 : 결측치에 대한 값을 별도로 정의
3. 전문가가 직접 값을 채우기 : 가장음

- 잡음 데이터 처리 방법
1. 잡음이란 : 랜덤 에러나 측정된 변수의 변형된 값
2. Bining : 데이터를 정렬한 다음 일정한 주파수 단위의 bins로 나누고 대표값으로 변환. 구간 단위별로 잡음 제거 및 데이터 축약 효과 (평균, Median 등)
3. Regression : 데이터를 가장 잘 표현하는 추세 함수를 찾아서 이 함수의 값을 사용
4. 클러스터링 : 비슷한 성격을 가진 클러스터 단위로 묶은 다음 outlier 제거

- 데이터 통합
1. 데이터 통합 : 다양한 소스로부터 얻은 데이터를 일관성 있는 하나의 데이터로 합치는 것
2. 스키마 통합 : 다양한 소스의 데이터의 메타데이터를 통합
3. 데이터 통합시의 문제 및 해결책
  - 중복 문제(같은 내용의 데이터가 다른 이름으로 들어가 있는 것) / 연관관계 분석 등을 통해서 중복 제거
  - 일관성 문제(계산, 통계를 통해 얻을 수 있는 값이 틀린 경우) / 계산 검증
  - Entity 확인 문제 (통합 대상 entry가 정말 동일한지 여부) / 검증 체크
  - 표현 문제 (kg 파운드) / 표현 일치화
  - 다른 스케일에 의한 통합 문제 / 스케일 변환 과정 필요

- 데이터 변환
1. Smoothing(평활화)
2. Aggregation(집합)
3. Generalization(일반화)
4. Normalization(정규화)
5. Attribute/feature construction

- 표준화
모집단이 정규분포를 따르는 경우에 N(0,1)인 표준정규분포로 표준화 하는 작업

- 정규화
전체구간을 0~100으로 설정하여 데이터를 관찰하는 방법

- 데이터 축소
1. 데이터 축소 : 적은 양으로도 전체 데이터 집합을 잘 대표하는 데이터 얻는 과정. 대규모 데이터의 작업 시 분석에 필요
2. 차원 축소 방법 : 여러 속성 중 분석하는데 관계없거나 중복되는 속성을 제거, 속성의 최소 집합을 찾음
3. 데이터 압축 : 데이터 인코딩이나 변환을 통해 데이터 축소. 손실 없이 다시 구할 수 있다면 lossless 기법, 데이터 손실이 있을 경우에는 lossy
4. Discrete wavelet transform (DWT) : 선형의 신호를 처리하는 기술, 데이터 평활화 작업 없이도 잡음 제거 효과
5. Principal components analysis (PCA) : 데이터를 가장 잘 표현하고 있는 직교상의 데이터 벡터들을 찾아서 데이터 압축
6. Numerosity Reduction (수량 축소) : 데이터를 더 작은 형태로 표현해서 데이터의 크기 줄임

- 특성을 추출을 사용한 차원 축소
feature로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것.
차원이 증가할수록 예측 신뢰도가 떨어지고, 과적합이 발생하고, 개별 feature 간의 상관관계가 높을 가능성이 있다.
PCA(주성분 분석)는 고차원의 데이터를 저차원의 데이터로 축소시키는 차원 축소 방법 중 하나
